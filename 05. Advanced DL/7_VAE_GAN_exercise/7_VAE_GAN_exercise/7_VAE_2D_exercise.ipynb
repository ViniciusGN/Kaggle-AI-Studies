{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7a - Variational Auto-Encoder in 2D\n",
    "\n",
    "The goal of a variational auto-encoder (VAE) is to learn a **generative model**, that is \n",
    "1. to learn a latent representation of the input data in a lower-dimensional space that is well structured, \n",
    "2. to be able to sample from this latent space to generate new data points that resemble the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## In a nutshell : VAE vs AE\n",
    "### Architecture\n",
    "A VAE is a type of neural network that consists of two main components: an **encoder** and a **decoder** : \n",
    "- The encoder $E : x \\in \\mathbb R^{d} \\to z = E(x) \\in \\mathbb R^{D} $ \n",
    "maps the input data to a latent space, \n",
    "- while the decoder $D : z \\in \\mathbb R^{D} \\to y = D(z) \\in \\mathbb R^{d} $\n",
    " maps points in the latent space back to the original data space. \n",
    "The key idea is to impose a probabilistic structure on the latent space, typically by assuming that the latent variables follow a Gaussian distribution : $z \\sim \\mathcal N(\\mu, \\Sigma)$ as it is easy to sample from (as seen earlier with GMM).\n",
    "\n",
    "In some sense, a VAE can be seen as a probabilistic extension of a standard auto-encoder (AE) combined with GMM, where the encoder outputs parameters of a probability distribution (mean and variance) instead of a single point in the latent space.\n",
    "\n",
    "### Training\n",
    "A VAE is trained by maximizing the **variational lower bound** on the log-likelihood of the data, which consists of two terms: the reconstruction loss (how well the decoder can reconstruct the input data from the latent representation) and a regularization term that encourages the latent space to follow a Gaussian distribution. This regularization is achieved by introducing a **Kullback-Leibler divergence** term that measures how much the learned latent distribution deviates from a prior distribution (usually a standard normal distribution).\n",
    "\n",
    "\n",
    "Here we implement a simple variational auto-encoder based on MLP using PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "Reference: \"Auto-encoding variational Bayes.\", Kingma, Diederik P., and Max Welling., Int. Conf. on Learning Representations (ICLR), 2014.\n",
    "\n",
    "julien rabin @ greyc.ensicaen.fr 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful imports, definitions and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n: int, model = 'radial_gmm') -> Tensor:\n",
    "    data_dim = 2  # Dimension of the data\n",
    "    if model == 'moons':\n",
    "        from sklearn.datasets import make_moons\n",
    "        return Tensor(make_moons(n_samples=n, noise=0.05)[0])\n",
    "    elif model == 'circles':\n",
    "        from sklearn.datasets import make_circles\n",
    "        return Tensor(make_circles(n_samples=n, noise=0.05, factor=0.5)[0])\n",
    "    elif model == '2gmm':\n",
    "        n_samples = n\n",
    "        n = n//3\n",
    "        X1 = torch.randn(n, data_dim) @ torch.tensor([[.05, -0.02],[-0.02, .4]]) + torch.tensor([[.0, 1.0]]).view(1, data_dim)\n",
    "        X2 = torch.randn(n_samples - n, data_dim) @ torch.tensor([[.3, 0.05],[0.05, .05]]) + torch.tensor([[-1.0, 0.]]).view(1, data_dim)\n",
    "        return torch.cat((X1,X2), dim=0)  # Concatenate the two sets of samples\n",
    "    elif model == 'radial_gmm':\n",
    "        K = 8\n",
    "        n_samples = n\n",
    "        samples_per_component = n_samples // K\n",
    "        remainder = n_samples % K\n",
    "        all_samples = []\n",
    "\n",
    "        for k in range(K):\n",
    "            radius = 3.\n",
    "            # Angle for the mean on a circle\n",
    "            theta = 2 * np.pi * k / K\n",
    "            cs = np.cos(theta)\n",
    "            sn = np.sin(theta)\n",
    "\n",
    "            # Radial direction unit vector\n",
    "            radial = torch.tensor([cs, sn], dtype=float)#.view(data_dim, 1)\n",
    "            tangential = torch.tensor([-sn, cs], dtype=float)#.view(data_dim, 1)\n",
    "\n",
    "            mean = radius * radial\n",
    "            \n",
    "            # Covariance matrix: elongated along radial direction\n",
    "            cov = 0.3 * torch.outer(radial, radial).to(float) + 0.05 * torch.outer(tangential, tangential).to(float)\n",
    "\n",
    "            # Generate samples\n",
    "            n = samples_per_component + (remainder if k == K else 0)\n",
    "            samples = torch.randn(n, data_dim, dtype=float) @ torch.linalg.cholesky(cov).to(float) + mean.to(float)\n",
    "            all_samples.append(samples)\n",
    "\n",
    "        return torch.cat(all_samples, dim=0).to(torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_comparison(x_data : np.ndarray, x_model : np.ndarray, colors = None):\n",
    "    if colors is None:\n",
    "        colors = 'C0' #np.random.rand(x_data.shape[0])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "    # Original data\n",
    "    axes[0].scatter(x_data[:, 0], x_data[:, 1], s=40, c=colors)\n",
    "    axes[0].set_title('Data Samples')\n",
    "    axes[0].set_xlim(-3.0, 3.0)\n",
    "    axes[0].set_ylim(-3.0, 3.0)\n",
    "\n",
    "    # VAE reconstruction\n",
    "    axes[1].scatter(x_model[:, 0], x_model[:, 1], s=40, c=colors)\n",
    "    axes[1].set_title('VAE Output Samples')\n",
    "    axes[1].set_xlim(-3.0, 3.0)\n",
    "    axes[1].set_ylim(-3.0, 3.0)\n",
    "\n",
    "    # Global title\n",
    "    fig.suptitle(\"Original vs Model samples\", fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_colors(x_data: np.ndarray ) -> np.ndarray:\n",
    "    cmap = plt.get_cmap(\"rainbow\")\n",
    "    values = x_data[:, 0]  # Use the first dimension for coloring\n",
    "    normalized = (values - values.min()) / (values.max() - values.min())\n",
    "    colors = cmap(normalized)\n",
    "    return colors[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Implementation of a pytorch VAE class \n",
    "\n",
    "Complete the following class implementing a VAE with MLP encoder and decoder.\n",
    "You can refer to the AE class implemented in the previous labs.\n",
    "\n",
    "Recall that \n",
    "1. the encoder $D$ should output both the **mean** and the **standard deviation** of the latent Gaussian distribution, that is \n",
    "$$D(x) = (\\mu(x), \\sigma(x)).$$\n",
    "Here, both $\\mu(x)$ and $\\sigma(x)$ are both vectors of size equal to the latent dimension (`latent_dim=1` by default).\n",
    "\n",
    "Note that in practice, for numerical stability (and to deal with positivity constraints), it is common to output the log-variance $\\log \\sigma^2(x)$ instead of the standard deviation $\\sigma(x)$ directly.\n",
    "\n",
    "2. the reparameterization trick should be used to sample from the latent distribution during training, that is\n",
    "$$z = \\mu(x) + \\sigma(x) * \\epsilon,$$\n",
    "where $\\epsilon \\sim \\mathcal N(0, I)$ is a standard normal random variable.\n",
    "3. the loss function should include both the reconstruction loss and the KL divergence term (see details later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, dim: int = 2, h: int = 64, latent_dim: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lat_dim = latent_dim\n",
    "        self.encoder = nn.Sequential( # output dimensions are batch_size x (2*latent_dim) for mean and logvar\n",
    "            nn.Linear(dim, 2*latent_dim)) # Output mean and logvar\n",
    "        \n",
    "        self.decoder = nn.Sequential( # input dimensions are batch_size x latent_dim\n",
    "            nn.Linear(latent_dim, dim))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor: # AE mode : encode then decode, using the reparameterization trick\n",
    "        # Note : x is of shape (batch_size, dim)\n",
    "        \n",
    "        # Extract mean and std from the output, both of shape (batch_size, latent_dim)\n",
    "        mu, std = ... # use 'encode_mu_std' method to get mu and std\n",
    "        \n",
    "        # Decode the latent space representation to data space: Output is of shape (batch_size, dim)\n",
    "        y = ... # use method 'decode_mu_std' \n",
    "        \n",
    "        return y # Note: this is the *mean* of the generated distribution in the data space\n",
    "    \n",
    "    def encode_mu_std(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        # Encode input x to latent space\n",
    "        out = self.encoder(x)\n",
    "        \n",
    "        mu  = out[:,:self.lat_dim]  # First half is the mean of the gaussian distribution\n",
    "        if True : # try predicting the standard deviation directly\n",
    "            std = nn.Softplus()(out[:,self.lat_dim:]) # Second half is the sandard deviation  > 0\n",
    "        else : # try predicting the log variance instead\n",
    "            std = ...\n",
    "        return mu, std\n",
    "    \n",
    "    def decode_mu_std(self, mu: Tensor, std : Tensor) -> Tensor:\n",
    "        # Decode latent space representation z to data space\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = ... # sample from the gaussian latent space using predicted parameters mu and std\n",
    "        \n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def sample(self, n: int, std : float = 0.) -> Tensor:\n",
    "        # Sample from the latent space distribution N(0, I)\n",
    "        z = ...\n",
    "        if std is None : # prediction is the mean of the distribution\n",
    "            return self.decoder(z)\n",
    "        else : # prediction is a gaussian distribution with std\n",
    "            y = self.decoder(z)\n",
    "            return y + ... # add noise according to std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the (untrained) VAE model : on data reconstruction\n",
    "\n",
    "Check that (like untrained AE) the untrained VAE model does not reconstruct well the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'radial_gmm' # moons, circles, 2gmm, radial_gmm\n",
    "x = ...  # Generate synthetic data\n",
    "\n",
    "# reconstruction of the training data\n",
    "model = ... # use VAE class to create model with different parameters\n",
    "y = model(x).detach()\n",
    "\n",
    "# define colors for the data points\n",
    "colors = define_colors(x.detach())\n",
    "\n",
    "fig, axes = plot_data_comparison(x,y, colors=colors)\n",
    "\n",
    "fig.suptitle(\"*Untrained* VAE reconstruction of data samples\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the (untrained) VAE model : generating samples by sampling the latent space\n",
    "\n",
    "Recall that an ideal VAE has a well-structured latent space, so that sampling from the prior distribution in the latent space should produce meaningful samples in the data space after decoding.\n",
    "\n",
    "In this lab we consider the usual prior distribution for the latent space, that is a standard normal distribution $\\mathcal N(0, I)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x = ...\n",
    "\n",
    "# Generate samples from the untrained VAE\n",
    "y = ... # use 'sample' method of the VAE model to generate samples\n",
    "\n",
    "fig, axes = plot_data_comparison(x,y)\n",
    "\n",
    "fig.suptitle(\"*UNTRAINED* VAE generation of random samples\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Principle of likelihood maximization for generative modeling with a decoder\n",
    "\n",
    "As seen with GMM and AE, the generative model itself is composed of two main components:\n",
    "- a latent space $z \\in \\mathcal Z$ that is (easily !) sampled from a prior distribution $p$ (usually a standard normal distribution in $\\mathbb R^h$) : from now on we assume $z \\sim p(z) = \\mathcal N(z|\\mu, \\Sigma)$\n",
    "- a parametric (here a Neural Network) decoder $D : z \\in \\mathcal Z \\to y = D(z) \\in \\mathcal X$ that maps points in the latent space to the data space $\\mathcal X$.\n",
    "This decoder can also be used to have a parametric model of the generated data $x'$ given $z$: again, a Gaussian distribution is often used, typically centered on the output of the decoder: $x'|z \\sim p_\\theta(x'|z) = \\mathcal N(x' | D(z), \\Sigma')$.\n",
    "\n",
    "Like GMM, the goal is to maximize the log-likelihood of the data $x \\sim p_{\\text{data}} \\in \\mathcal X$ with the generative model parametrized by $\\theta$.\n",
    "This can be expressed as (for a single data point $x$):\n",
    "$$\n",
    "    \\ell(\\theta, x) = \\log p_{\\theta}(x) = \\log \\int p_{\\theta}(x|z) p(z) dz = \\log \\int p_{\\theta}(x|z) p(z) dz\n",
    "    = \\log \\mathbb E_{z \\sim p(z)}[p_\\theta(x|z)]\n",
    "$$\n",
    "where $\\theta$ are the parameters of the decoder $D$ and the prior distribution $p(z)$.\n",
    "To train te model we have to maximize the log-likelihood over training samples from the data distribution $p_{\\text{data}}$:\n",
    "$$\n",
    "    \\mathcal L(\\theta) = \\mathbb E_{x\\sim p_{\\text{data}}} \\ell(\\theta, x) = \\mathbb E_{x\\sim p_{\\text{data}}} \\log \\mathbb E_{z \\sim p(z)}[p_\\theta(x|z)]\n",
    "$$\n",
    "\n",
    "However, this is **intractable**:\n",
    "- the integral cannot be computed analytically because the conditional distribution $p_{\\theta}(x|z)$, parametrized by the decoder, is typically complex and high-dimensional ! \n",
    "\n",
    "*Note: Some models we consider later adresse this specific problem by considering particular classes of neural networks (e.g. Normalizing FLow, discrete autoregressive model) that allow compute the likelihood of the data point $x$ given the latent variable $z$ in a tractable way, but this is not the case in general*\n",
    "\n",
    "- the latent variables $z$ corresponding to data points $x$ are not observed, so we cannot directly sample from them to compute the expectation,\n",
    "\n",
    "*Note that the posterior distribution $p_{\\theta}(z|x) = \\frac{p_{\\theta}(x|z)p(z)}{p_{data}(x)}$ (by Bayes's rule) is also complex and high-dimensional ...*\n",
    "\n",
    "**Idea:** to make the problem tractable, try to **approximate** the posterior distribution $p_{\\theta}(z|x)$ with a simpler distribution $q_\\phi(z|x)$, where $\\phi$ are the parameters of an encoder $E$ which is trained to infer the unobserved latent variable $z$ from a data point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle of VAE for generative modeling\n",
    "\n",
    "- **Key idea #1:** To address this issue, we use a **variational approach**: we introduce a variational distribution $q_\\phi(z|x)$ (parametrized by an encoder) that approximates the true posterior distribution $p(z|x)$ of the latent variables given the data. The goal is to find the parameters $\\phi$ of this variational distribution that minimize the Kullback-Leibler divergence between $q_\\phi(z|x)$ and $p(z|x)$.\n",
    "\n",
    "- **Key idea #2:** As stated before, even knowing the posterior distribution $p(z|x)$, the integral $\\int p_{\\theta}(x|z) p(z) dz$ is still intractable, so we cannot compute the log-likelihood of the data directly. Instead, we can use the **variational lower bound** on the log-likelihood of the data, which can be expressed as (*see the course for details*) for any distribution $q(z)$:\n",
    "$$\n",
    "    \\log p_\\theta(x) \n",
    "    = \\mathbb E_{z \\sim q(z)} \\left[ \\log p_{\\theta}(x) \\right] \n",
    "    \\ge \\mathbb E_{z \\sim q(z)} \\left[ \\frac{\\log p_{\\theta}(x,z)}{q(z)} \\right] \n",
    "$$\n",
    "\n",
    "Using $q(z) = q_\\phi(z|x)$ yields (*see the course for details*) the following loss function to maximize:\n",
    "$$\\begin{align*}\n",
    "    \\mathcal L(\\theta, \\phi) &= \\mathbb E_{x\\sim p_{\\text{data}}} \\left[ \\mathbb E_{z \\sim q_\\phi(z|x)} \\left[ \\log p_{\\theta}(x|z) - \\text{KL}(q_\\phi(z|x) || p(z)) \\right] \\right] \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "- **Key idea #3 (reparametrization trick):** to make the computation of each term tractable, we use a simple gaussian parametrization of the involved conditional dentities: as stated before we can impose for instance $p_{\\theta}(x|z) = \\mathcal N(x' | D(z), \\Sigma)$ with $\\Sigma= \\sigma^2 I_d$ a constant covariance matrix.\n",
    "Same for the posterior distribution $q_\\phi(z|x)$ (that is arbitrary), we can use a Gaussian distribution with mean $\\mu_\\phi(x)$ and **diagonal covariance** matrix $\\Sigma_\\phi(x) = \\sigma_\\phi^2(x) I_D$, where this time both $\\mu_\\phi$ and $\\sigma_\\phi$ are neural networks (encoders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**: During training, one can use batch optimization techniques such as stochastic gradient descent (SGD) or Adam to optimize the parameters $\\theta$ and $\\phi$ of the decoder and encoder, respectively. \n",
    "To do so, we need to sample a batch $\\{x_i\\}$ from the data distribution $p_{\\text{data}}$ but also sample $z_i$ from the variational distribution $q_\\phi(z|x_i)$ for each data point $x_i$.\n",
    "TO so so\n",
    "\n",
    "After sampling $x_i$, instead of sampling $z_i$ directly from $q_\\phi(z|x)$, we sample random variables $\\epsilon_i$ from a standard distribution $\\mathcal N(0,I_D)$ and then apply a deterministic transformation to obtain $z$ that depends on the data point $x$ and the parameters $\\phi$:\n",
    "$$\n",
    "    z_i = \\mu_\\phi(x_i) + \\sigma_\\phi(x_i) \\odot \\epsilon_i\n",
    "$$\n",
    "\n",
    "These two samplings allows us to backpropagate through the sampling process and optimize the parameters of the encoder and decoder jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss evaluation**\n",
    "Using samples $(z_i,x_i)$, the loss function used during training is a combination of two terms:\n",
    "\n",
    "- **reconstruction loss** (MSE): measures how well the probabilistic model $p_{\\theta}(x|z)$ can reconstruct the input data $x$ from the latent variable $z$ (predicted using the surrogate posterior $q_\\phi(z|x)$), that is how close $y_i=D_{\\theta}(z_i)$ is from the input data $x_i$, using the *negative* log-likelihood of a $d$-dimensional gaussian distribution\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\mathcal L_{\\text{rec}} (\\theta,\\phi) \n",
    "\t&= - \\mathbb E_{x \\sim p_{data}} \\mathbb E_{z \\sim q_{\\phi}(z|x)} \\log p_\\theta(x|z) \n",
    "\t\\\\\n",
    "\t&= \\mathbb E_{x \\sim p_{data}} \\mathbb E_{z \\sim q_{\\phi}(z|x)} \\tfrac{1}{2} \\left(\\tfrac{1}{\\sigma^2}  \\| D_{\\theta}(z_i) - x \\|^2 + d\\log(2\\pi) + d\\log(\\sigma^2) \\right)\n",
    "\t\\\\\n",
    "\t&\\approx \\tfrac{1}{2N\\sigma^2}   \\sum_{i=1}^N \\| D_{\\theta}(z_i) - x_i \\|^2 \n",
    "\t\\;+\\; \\text{Cst}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\theta$ and $\\sigma^2$ are the parameters of the gaussian distribution that models the reconstruction of $x$ with some variance $\\sigma^2$.\n",
    "Remember that the encoder $E$ outputs the mean $\\mu_\\phi(x_i)$ and the standard deviation $\\sigma_\\phi(x_i)$ of the latent variable $z_i = \\mu_\\phi(x_i) + \\sigma_\\phi(x_i) \\epsilon_i$, so the reconstruction loss both depends on the parameters of the decoder $D$ and the encoder $E$ !\n",
    "\n",
    "In practice, for a batch of N samples $(x_i, \\epsilon_i) \\sim p_{\\text{data}} \\times \\mathcal N(0,I_d)$ (ignoring the constant terms), we can write the reconstruction loss as a MSE loss:\n",
    "$$\n",
    "\t\\mathcal L_{\\text{rec}} (\\theta,\\phi) \n",
    "\t= \\tfrac{\\lambda}{2} MSE(x,D_\\theta(\\mu_\\phi(x_i) + \\sigma_\\phi(x_i) \\epsilon_i))\n",
    "$$\n",
    "and the corresponding weight for the reconstruction loss is given by the inverse of the variance:\n",
    "$$\n",
    "\t\\lambda = \\frac{1}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "- **Kullback-Leibler (KL) divergence:** measures how well the latent space distribution matches a prior distribution (here the standard normal distribution in $h$ dimensions)\n",
    "\n",
    "$$\\text{KL} \\left( {\\mathcal N}(\\mu,\\sigma^2) \\,||\\, {\\mathcal N}(0,1) \\right) \n",
    "\t= \\tfrac{1}{2} \\left(\\|\\mu\\|^2 + \\|\\sigma\\|^2  - \\sum_i (\\log (\\sigma_i^2) +1) \\right)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\mathcal L_{\\text{KL}} (\\theta,\\phi) \n",
    "\t&= \\mathbb E_{x \\sim p_{data}} \\text{KL} \\left( q_\\phi(z|x) \\,||\\, p(z) \\right)\n",
    "\t\\\\\n",
    "\t&\\approx \\tfrac{1}{2N} \\sum_{i=1}^N \\left( \\|\\mu_\\phi(x_i)\\|^2 + \\|\\sigma_\\phi(x_i)\\|^2 - 2 \\langle  \\sigma_{\\phi}(x_i), 1_h\\rangle - h) \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that to ensure that the KL divergence is well defined, we need to ensure that the standard-deviation $\\sigma_\\phi(x_i)>0$ is positive, which can be simply achieved by different means. In the following we use an exponential activation function $\\sigma_\\phi(x_i)=\\exp(\\text{MLP}(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Implement the VAE loss function and train a VAE model\n",
    "\n",
    "like previous labs, experiment with different values of the reconstruction weight $\\lambda$ (`lambda_rec`) (related to the variance of the gaussian reconstruction) and latent dimension `latent_dim`, starting from `latent_dim=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VAE model and training parameters\n",
    "\n",
    "torch.manual_seed(0) # For reproducibility\n",
    "\n",
    "latent_dim = 1\n",
    "model = ...\n",
    "\n",
    "lambda_rec = ...  # Reconstruction loss weight (Note: related to the variance of the gaussian reconstruction)\n",
    "\n",
    "learn_rate = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "batch_size = 256\n",
    "\n",
    "rec_losses = []\n",
    "KL_losses = []\n",
    "\n",
    "pbar = tqdm(range(n_iter), desc=\"Training VAE\")\n",
    "for _ in pbar :\n",
    "    x = ...\n",
    "    \n",
    "    # Note : we use only once the encoder and the decoder for the same batch\n",
    "    mu, std = ...\n",
    "    y = ...\n",
    "    var = std**2 \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    rec_loss = ... # Reconstruction loss : sum of squared errors (not mean !)\n",
    "    KL_loss = ...  # KL divergence term between gaussians in latent space (see formula above !)\n",
    "    loss = lambda_rec * rec_loss + KL_loss  # Total loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    rec_losses.append(rec_loss.item())\n",
    "    KL_losses.append(KL_loss.item())\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'total loss': f\"{loss.item():.4f}\",\n",
    "        'SSE loss'  : f\"{rec_losses[-1]:.4f}\",\n",
    "        'KL loss'  : f\"{KL_losses[-1]:.4f}\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.semilogy(rec_losses, label='Reconstruction Loss', color='blue')\n",
    "ax.semilogy(KL_losses, label='KL Divergence Loss', color='orange')\n",
    "ax.semilogy([lambda_rec * x + y for x, y in zip(rec_losses, KL_losses)], label='Total Loss (weighted)', color='green')\n",
    "ax.set_title('Losses during VAE Training')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss Value')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction with a trained VAE model\n",
    "\n",
    "Complete the following code to reconstruct training data with the trained VAE model.\n",
    "Check that the reconstruction is better than with an untrained model.\n",
    "Is it better than an AE model and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the training data\n",
    "x = ...\n",
    "\n",
    "# reconstruction of the training data : using only the mean of the learned distribution\n",
    "y = ... # output of the model on input x is the mean of the generated distribution\n",
    "\n",
    "colors = define_colors(x.detach())\n",
    "fig, axes = plot_data_comparison(x,y, colors=colors)\n",
    "fig.suptitle(\"VAE reconstruction of data samples\", fontsize=24)\n",
    "\n",
    "# reconstruction of the training data with noise\n",
    "std = 1/np.sqrt(lambda_rec)\n",
    "y_noised = ... # add noise with standard deviation 'std' to 'y'\n",
    "fig, axes = plot_data_comparison(x,y_noised, colors=colors)\n",
    "fig.suptitle(\"stochastic VAE reconstruction with gaussian noise\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent distribution of a trained VAE model\n",
    "\n",
    "The purpose of the VAE is to learn a well-structured latent space, so that sampling from the prior distribution in the latent space should produce meaningful samples in the data space after decoding.\n",
    "\n",
    "1. In the following cells, we visualize the learned **latent parameters** by plotting the mean and standard deviation of the latent variables for each data point in the training set and check their distribution.\n",
    "\n",
    "What do we expect for these distributions ? what do you observe ?\n",
    "\n",
    "2. We want to visualize the latent variables $z$ encoded by the VAE model.\n",
    "\n",
    "What do we expect for this distribution ? what do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the training data\n",
    "n = ...\n",
    "x = ...\n",
    "\n",
    "# reconstruction of the training data\n",
    "mu, std = ...\n",
    "eps = ... # random gaussian noise\n",
    "z = ...  # corresponding random latent representation of x\n",
    "\n",
    "eps = eps.detach().numpy()\n",
    "z = z.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize the latent space parameters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=False, sharey=False)\n",
    "\n",
    "if mu.shape[1] == 1 :  # only if latent dimension is 1\n",
    "    axes[0].hist(mu.detach()[:, 0], bins=30, color='C0')\n",
    "    axes[0].set_title(f'VAE latent mean')\n",
    "    axes[0].legend()\n",
    "    axes[1].hist(std.detach()[:, 0], bins=30, color='C1')\n",
    "    axes[1].set_title(f'VAE latent std')\n",
    "    axes[1].legend()\n",
    "else :\n",
    "    axes[0].scatter(0, 0, s=100, color='red', label='mean target')\n",
    "    axes[0].scatter(mu.detach()[:, 0], mu.detach()[:, 1], s=40, color='C0')\n",
    "    axes[0].set_title(f'VAE latent mean (first 2 dimensions)')\n",
    "    axes[0].legend()\n",
    "    axes[1].scatter(1, 1, s=100, color='red', label='std target')\n",
    "    axes[1].scatter(std.detach()[:, 0], std.detach()[:, 1], s=40, color='C1')\n",
    "    axes[1].set_title(f'VAE latent std (first 2 dimensions)')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.suptitle('VAE latent mean and std variables (1st 2 dimensions)')\n",
    "plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize the latent space distribution vs expected standard gaussian\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "if mu.shape[1] == 1 :  # only if latent dimension is 1\n",
    "    axes[0].hist(eps[:, 0], bins=30, color='red')\n",
    "    axes[0].set_title(f'standard gaussian latent space')\n",
    "    axes[0].set_xlim(-3.0, 3.0)\n",
    "\n",
    "    axes[1].hist(z[:, 0], bins=30, color='blue')\n",
    "    axes[1].set_title(f'VAE latent')\n",
    "    axes[1].set_xlim(-3.0, 3.0)\n",
    "    \n",
    "else : # latent dimension is 2 or more\n",
    "    axes[0].scatter(eps[:, 0], eps[:, 1], s=40, color='red')\n",
    "    axes[0].set_title(f'standard gaussian latent space (first 2 dimensions)')\n",
    "    axes[0].set_xlim(-3.0, 3.0)\n",
    "    axes[0].set_ylim(-3.0, 3.0)\n",
    "\n",
    "    axes[1].scatter(z[:, 0], z[:, 1], s=40)\n",
    "    axes[1].set_title(f'VAE latent (first 2 dimensions)')\n",
    "    axes[1].set_xlim(-3.0, 3.0)\n",
    "    axes[1].set_ylim(-3.0, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling random samples with a trained VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = 1_000\n",
    "\n",
    "x = sample_data(n=n, model = dataset_name)\n",
    "\n",
    "# Generate samples from the trained VAE\n",
    "y = model.sample(n).detach()\n",
    "\n",
    "fig, axes = plot_data_comparison(x,y)\n",
    "\n",
    "fig.suptitle(\"VAE generation of random samples with std=0\", fontsize=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with variance using var = 1/lambda_rec\n",
    "torch.manual_seed(42)\n",
    "n = 1_000\n",
    "\n",
    "x = ...\n",
    "\n",
    "# Generate samples from the trained VAE\n",
    "std = 1/np.sqrt(lambda_rec)\n",
    "print(f\"Sampling with std = {std:.2f}\")\n",
    "y = ...\n",
    "\n",
    "fig, axes = plot_data_comparison(x,y)\n",
    "\n",
    "fig.suptitle(\"VAE generation of random samples with var=1/\\lambda_rec\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Discussion\n",
    "\n",
    "In this notebook, we have implemented a simple Variational Auto-Encoder (VAE) using PyTorch. \n",
    "The generative model is composed of a decoder that maps a small latent space with a simple prior distribution to the (high-dimensional) data space.\n",
    "To train such model the VAE requires an encoder that maps implicitely the training data to the latent space: the decoder yields paramters of a Gaussian distribution (mean and variance) that are then used to sample from the latent space ('reparametrization trick'). \n",
    "The auto-encoder is learnt using a combination of reconstruction loss and KL divergence to ensure that the latent space follows the desired distribution (here a standard Gaussian).\n",
    "\n",
    "List the Advantages and Drawbacks of the VAE compared to a simple auto-encoder (AE) and GMM :\n",
    "- ✅ ...\n",
    "- ❌ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebooks, we will explore more advanced generative models that address some of these limitations, such as Generative Adversarial Networks (GANs) and Diffusion Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_demo_deep_gen_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
