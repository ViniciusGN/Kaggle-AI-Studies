{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7b - Generative Adversarial Networks (GANs) in 2D\n",
    "\n",
    "The goal of a Generative Adversarial Networks (GANs) is to learn a **generative model**, where\n",
    "1. the low-dimensional latent space distribution $p$ is explicitely fixed;\n",
    "2. the quality of the generated samples is evaluated by an auxilliary model (classification network) driving the optimization of the generative model (decoder network).\n",
    "\n",
    "This is in contrast to **Variational Auto-Encoders (VAEs)** seen previously where the latent representation is optimized to during training to match the desired distribution, and the quality of the generated samples is evaluated by the reconstruction loss that is agnostic to the data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "Reference: \"Generative adversarial Networks\", Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua. In Advances in neural information processing systems (NeuRIPS) 2014.\n",
    "\n",
    "julien (dot) rabin @ greyc.ensicaen.fr 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick recap on GANs\n",
    "\n",
    "### Architecture\n",
    "A GAN is a type of neural network that consists of two main components: a generative network (a decoder called **generator G**) used exclusively during inference, and a discriminative network (a **classifier**) used only during training. The generative network learns to generate new data samples that resemble the training data, while the discriminative network learns to distinguish between real data samples and those generated by the generative network: \n",
    "\n",
    "- The generator $G : z \\in \\mathbb R^{D} \\to y = G(z) \\in \\mathbb R^{d} $ maps points in the latent space to the original data space. \n",
    "- The discriminator $D : x \\in \\mathbb R^{d} \\to y = D(x) \\in [0,1] $ \n",
    "maps the input data to a scalar value between 0 and 1, representing the probability that the input $x$ is a real data sample (from the training distribution $p_{\\text{data}}$) rather than a generated sample from the distribution $p_G := G_\\# p$.\n",
    "\n",
    "Like many other generative model, GAN typically by impose that the latent variables follow a Gaussian distribution : $z \\sim \\mathcal N(0, I_D)$ as it is easy to sample from (as seen earlier with GMM and VAE).\n",
    "\n",
    "The key idea of GAN is to train the generator and discriminator in a **minimax game**:\n",
    "- The generator tries to minimize the probability of the discriminator correctly identifying generated samples as fake, i.e., it tries to maximize the discriminator's error.\n",
    "- The discriminator tries to maximize its ability to correctly classify real and generated samples, i.e., it tries to minimize its error.\n",
    "\n",
    "### Training\n",
    "\n",
    "Contrary to VAEs and GMMs, GANs are not based on a probabilistic model optimized by maximizing the likelihood of the data. Instead, they are trained using a minimax game where the generator and discriminator are trained jointly in an adversarial manner.\n",
    "\n",
    "To the point of view of the discriminator, the problem is similar to binary classification problem\n",
    "where the criterion is the **binary cross-entropy loss** between the predicted probabilities $p_i = D(x_i) \\in [0,1]$ and the true labels $y_i \\in \\{0,1\\}$.\n",
    "Though, contrary to a conventional classification problem, this problem does not require supervision: labels can be automatically defined (e.g. 1 for real samples: $x \\sim p_{\\text{data}}$, 0 for generated samples $x \\sim p_{\\text{G}}$) which makes it an **unsupervised learning** problem:\n",
    "$$  \n",
    "   \\min \\mathcal L (D) := - \\mathbb E_{x \\sim p_{\\text{data}}} \\left[ \\log D(x) \\right] - \\mathbb E_{x' \\sim p_G} \\left[ \\log (1 - D(x')) \\right]\n",
    "$$\n",
    "\n",
    "Now, considering the fact that the distribution of generated samples $p_G = G_\\# p$ is parametrized by the generator $G$ (the prior latent distribution $p$ is fixed), we can rewrite the loss function as a function of the generator parameters, which role is to maximize the discriminator's error:\n",
    "$$  \n",
    "     \\min_{D} \\max_{G} \\mathcal L (G,D) = - \\mathbb E_{x \\sim p_{\\text{data}}} \\left[ \\log D(x) \\right] - \\mathbb E_{z \\sim p} \\left[ \\log (1 - D(G(z))) \\right]\n",
    "$$\n",
    "This way, the quality of the generated samples is automatically evaluated by the discriminator, which drives the optimization of the generator.\n",
    "\n",
    "To train the GAN, we alternate between two steps:\n",
    "1. **Train the discriminator**: Update the discriminator parameters $D$ by minimizing the loss $ \\mathcal L (D) $.\n",
    "\n",
    "*Note that the discriminator should have good performance to be relevant, so it is often trained for several iterations before updating the generator.*\n",
    "\n",
    "2. **Train the generator**: Update the generator parameters $G$ by maximizing the loss $\\mathcal L (G,D)$ which reduces to minimizing the negative log-likelihood of the generated samples being classified as fake by the discriminator:\n",
    "$\\mathbb E_{z \\sim p} \\left[ \\log (1 - D(G(z))) \\right] $\n",
    "\n",
    "*Note that only randomly generated samples are now required to optimize the generator !*\n",
    "\n",
    "\n",
    "### Implementation\n",
    "Here we implement a simple GAN generator and discriminator based on MLPs using PyTorch.\n",
    "\n",
    "First we review the training of a simple MLP classifier to discriminate between the data distribution and an untrained generative model.\n",
    "Then, we train the generator in adversarial manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful imports, definitions and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n: int, model = 'moons') -> Tensor:\n",
    "    data_dim = 2  # Dimension of the data\n",
    "    if model == 'moons':\n",
    "        from sklearn.datasets import make_moons\n",
    "        return Tensor(make_moons(n_samples=n, noise=0.05)[0])\n",
    "    elif model == 'circles':\n",
    "        from sklearn.datasets import make_circles\n",
    "        return Tensor(make_circles(n_samples=n, noise=0.05, factor=0.5)[0])\n",
    "    elif model == '2gmm':\n",
    "        n_samples = n\n",
    "        n = n//3\n",
    "        X1 = torch.randn(n, data_dim) @ torch.tensor([[.05, -0.02],[-0.02, .4]]) + torch.tensor([[.0, 1.0]]).view(1, data_dim)\n",
    "        X2 = torch.randn(n_samples - n, data_dim) @ torch.tensor([[.3, 0.05],[0.05, .05]]) + torch.tensor([[-1.0, 0.]]).view(1, data_dim)\n",
    "        return torch.cat((X1,X2), dim=0)  # Concatenate the two sets of samples\n",
    "    elif model == 'radial_gmm':\n",
    "        K = 8\n",
    "        n_samples = n\n",
    "        samples_per_component = n_samples // K\n",
    "        remainder = n_samples % K\n",
    "        all_samples = []\n",
    "\n",
    "        for k in range(K):\n",
    "            radius = 3.\n",
    "            # Angle for the mean on a circle\n",
    "            theta = 2 * np.pi * k / K\n",
    "            cs = np.cos(theta)\n",
    "            sn = np.sin(theta)\n",
    "\n",
    "            # Radial direction unit vector\n",
    "            radial = torch.tensor([cs, sn], dtype=float)#.view(data_dim, 1)\n",
    "            tangential = torch.tensor([-sn, cs], dtype=float)#.view(data_dim, 1)\n",
    "\n",
    "            mean = radius * radial\n",
    "            \n",
    "            # Covariance matrix: elongated along radial direction\n",
    "            cov = 0.3 * torch.outer(radial, radial).to(float) + 0.05 * torch.outer(tangential, tangential).to(float)\n",
    "\n",
    "            # Generate samples\n",
    "            n = samples_per_component + (remainder if k == K else 0)\n",
    "            samples = torch.randn(n, data_dim, dtype=float) @ torch.linalg.cholesky(cov).to(float) + mean.to(float)\n",
    "            all_samples.append(samples)\n",
    "\n",
    "        return torch.cat(all_samples, dim=0).to(torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define useful plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_comparison(x_data : np.ndarray, x_model : np.ndarray, colors_data = None, colors_model = None):\n",
    "    if colors_data is None:\n",
    "        colors_data = 'C0' # blue\n",
    "        colors_model = 'C1' # orange\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "    # Original data\n",
    "    axes[0].scatter(x_data[:, 0], x_data[:, 1], s=10, c=colors_data)\n",
    "    axes[0].set_title('Data Samples')\n",
    "    axes[0].set_xlim(-3.0, 3.0)\n",
    "    axes[0].set_ylim(-3.0, 3.0)\n",
    "\n",
    "    # GAN samples\n",
    "    axes[1].scatter(x_model[:, 0], x_model[:, 1], s=10, c=colors_model)\n",
    "    axes[1].set_title('GAN Output Samples')\n",
    "    axes[1].set_xlim(-3.0, 3.0)\n",
    "    axes[1].set_ylim(-3.0, 3.0)\n",
    "\n",
    "    # Global title\n",
    "    fig.suptitle(\"Original vs Model samples\", fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_colors(x_data : np.ndarray, x_label : np.ndarray) -> np.ndarray:\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    values = x_label  # Use the label for coloring\n",
    "    values = (values - values.min()) / (values.max() - values.min())\n",
    "    colors = cmap(values)\n",
    "    return colors[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 : definition of the Neural Networks Architecture\n",
    "\n",
    "Complete the following code to define the generator and discriminator architectures as simple MLPs with hidden layers.\n",
    "\n",
    "Recall that the discriminator outputs \n",
    "- either a a scalar probability in [0,1], which can be obtained using a sigmoid activation in the last layer. Such a classifier can be trained using the binary cross-entropy loss : `nn.BCELoss()`\n",
    "- or a logit (real value) without any activation in the last layer. Such a classifier can be trained using the binary cross-entropy with logits loss : `nn.BCEWithLogitsLoss()`\n",
    "\n",
    "In this lab, we will use the **second option**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, dim: int = 2, h: int = 64, latent_dim: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lat_dim = latent_dim\n",
    "        \n",
    "        # Classifier / Critique\n",
    "        self.discriminator = ...\n",
    "        \n",
    "        # Decoder\n",
    "        self.generator = ...\n",
    "    \n",
    "    def discriminate(self, x: Tensor) -> Tensor:\n",
    "        # Note : input x is a batch of fake or real points in the data space of shape (batch_size, dim)\n",
    "        return self.discriminator(x)\n",
    "    \n",
    "    def sample(self, n: int) -> Tensor:\n",
    "        # Sample from the latent space distribution N(0, I)\n",
    "        z = ...\n",
    "        y = self.generator(z)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the (untrained) GAN : generate random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random GAN model\n",
    "latent_dim = 1\n",
    "model = GAN(dim=2, h=16, latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trainable parameters\n",
    "for name, net in [(\"Generator\",model.generator), ('Discriminator', model.discriminator)]:\n",
    "    print(f\"Network: {name}\")\n",
    "    n_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters in {net.__class__.__name__}: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = 1_000  # Number of samples\n",
    "dataset_name = 'moons' # moons, circles, 2gmm, radial_gmm\n",
    "\n",
    "# Generate synthetic data\n",
    "x = ...\n",
    "\n",
    "# Generate samples from the untrained GAN\n",
    "y = ...\n",
    "\n",
    "# use the discriminator logits to label the samples\n",
    "x_label = nn.Sigmoid()(model.discriminate(x)).detach().numpy() # sigmoid is used to get probabilities in [0,1]\n",
    "y_label = nn.Sigmoid()(model.discriminate(y)).detach().numpy()\n",
    "\n",
    "x_colors = define_colors(x.numpy(), x_label)  \n",
    "y_colors = define_colors(y.numpy(), y_label)  \n",
    "fig, axes = plot_data_comparison(x,y, x_colors, y_colors)\n",
    "\n",
    "fig.suptitle(\"*UNTRAINED* GAN generation of random samples\", fontsize=24)\n",
    "fig.colorbar(axes[0].collections[0], ax=axes[0], label='Data Probability', orientation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with a 2D classifier (discriminator), it is interesting to visualize its decision boundary (before, after but also during training !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_GAN_critic (gan : GAN) : # plot training data + GAN samples with discriminator values in background\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    n = 100  # Number of samples\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    x = ...\n",
    "    # Generate samples from the untrained GAN\n",
    "    y = ...\n",
    "\n",
    "    n = 100 # n**2 points for the grid\n",
    "    t = np.linspace(-3, 3, n)\n",
    "    X,Y = np.meshgrid(t, t)\n",
    "    Z = gan.discriminate(Tensor(np.c_[X.ravel(), Y.ravel()])) # in R\n",
    "    Z = nn.Sigmoid()(Z).detach().numpy() # convert to probability in [0,1]\n",
    "    Z = Z.reshape(X.shape)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5), sharex=True, sharey=True)\n",
    "    axes = [axes]\n",
    "    # Decision boundary \n",
    "    axes[0].contourf(X, Y, Z, levels=50, cmap='coolwarm', alpha=0.5)\n",
    "    fig.colorbar(axes[0].collections[0], ax=axes[0], label='Discriminator Output', orientation='horizontal')\n",
    "   \n",
    "    # Original data points\n",
    "    axes[0].scatter(x[:, 0], x[:, 1], s=10, alpha=0.5, c='C1', label='Data Samples')\n",
    "    axes[0].set_xlim(-3.0, 3.0)\n",
    "    axes[0].set_ylim(-3.0, 3.0)\n",
    "    \n",
    "    # GAN samples points\n",
    "    axes[0].scatter(y[:, 0], y[:, 1], s=10, alpha=0.5, c='C2', label='GAN Samples')\n",
    "    axes[0].set_title('Data and GAN Samples')\n",
    "    axes[0].legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_GAN_critic(model)\n",
    "axes[0].set_title(\"*UNTRAINED* GAN Critic Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this result what do you expect from an untrained discriminator ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-Up : training the discriminator only\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the classifier neural network gives logits as output (batch of real values) rather than probabilities, so we need to apply the sigmoid function to get probabilities.\n",
    "\n",
    "We use here directly the `torch.nn.BCEWithLogitsLoss` loss function which combines a sigmoid layer and the binary cross-entropy loss in one single class. This is more numerically stable than using a plain Sigmoid followed by a BCELoss when the input is very large (or very small).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = ...\n",
    "learning_rate = ...\n",
    "optimizer_dis = ... # setup the optimizer for the discriminator\n",
    "criterion = ...\n",
    "dis_losses = []\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for _ in tqdm(range(n_iter), desc=\"Training Discriminator\"):\n",
    "    # Sample/predict/label real data\n",
    "    x = ...\n",
    "    x_pred = ... # use the discriminator to predict on real data\n",
    "    x_label = torch.ones((batch_size, 1))  # Real labels are 1\n",
    "    \n",
    "    # Sample/predict/label fake data (from untrained generator)\n",
    "    y = ... # create fake data samples\n",
    "    y_pred = ...\n",
    "    y_label = torch.zeros((batch_size, 1))  # Fake labels are 0\n",
    "    \n",
    "    # Concatenate real and fake data\n",
    "    xy_pred = torch.vstack((x_pred, y_pred))\n",
    "    labels = torch.vstack((x_label, y_label))\n",
    "    \n",
    "    # Compute BCE loss\n",
    "    loss = criterion(xy_pred, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer_dis.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_dis.step()\n",
    "    \n",
    "    dis_losses.append(loss.item())\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "axes.plot(dis_losses, label='Discriminator Loss', color='C0')\n",
    "axes.set_title('Discriminator Loss Over Iterations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary and the data/samples colored by the *trained* discriminator output probabilities. \n",
    "\n",
    "Is it what you expect from a trained discriminator ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = ...\n",
    "x = ...\n",
    "\n",
    "y = ...\n",
    "\n",
    "x_label = nn.Sigmoid()(model.discriminate(x)).detach().numpy()\n",
    "y_label = nn.Sigmoid()(model.discriminate(y)).detach().numpy()\n",
    "\n",
    "x_colors = define_colors(x.numpy(), x_label)  \n",
    "y_colors = define_colors(y.numpy(), y_label)  \n",
    "\n",
    "fig, axes = plot_data_comparison(x,y, x_colors, y_colors)\n",
    "\n",
    "fig.suptitle(\"Untrained generator with trained discriminator\", fontsize=24)\n",
    "fig.colorbar(axes[0].collections[0], ax=axes[0], label='Data Probability', orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_GAN_critic(model)\n",
    "axes[0].set_title(\"Untrained Generator with Trained Discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Training the full GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem is a min max problem w.r.t. to the generator $G$ and the discriminator $D$:\n",
    "$$\\min_G \\max_D \\mathbb E_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb E_{z \\sim p_z} [\\log (1 - D(G(z)))]$$\n",
    "\n",
    "where $p_{\\text{data}}$ is the distribution of the training data and $p_z = \\mathcal N (0_d,I_d) $ is the d-dimensional latent distribution (input to the generator).\n",
    "\n",
    "For a fixed generator $G$, the discriminator $D$ is trained to maximize the probability of assigning the correct label to both real (1) and generated samples (0), which is @equivalent to minimizing the Binary Cross Entropy (BCE) loss of the mixture of real (x, label 1) and generated (y, label 0) samples:\n",
    "$$\n",
    "\t\\min_D - \\mathbb E_{x \\sim p_{\\text{data}}} [\\log D(x)] - \\mathbb E_{y \\sim p_G} [\\log (1 - D(y))]\n",
    "$$\n",
    "\n",
    "Using the (fixed) discriminator $D$ as a criterion, the generator $G$ is trained to minimize the probability of the discriminator assigning the wrong label to generated samples :\n",
    "$$\n",
    "\t\\min_G \\mathbb E_{y \\sim p_G} [\\log (1 -  D(y))]\n",
    "$$\n",
    "To avoid vanishing gradients (a good discriminator will provide $D(y)\\approx 0$), the generator is equivalently trained to maximize the probability of the discriminator assigning the desired label to generated samples (i.e. 1):\n",
    "$$\n",
    "\t\\min_G - \\mathbb E_{y \\sim p_G} [\\log D(y)]\n",
    "$$\n",
    "This is equivalent to minimizing the BCE loss of the generated samples $y$ but using the desired label 1 (instead of 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # For reproducibility\n",
    "\n",
    "model = ... # create a new GAN model\n",
    "criterion = ...\n",
    "\n",
    "learning_rate = ...\n",
    "optimizer_gen = ... # setup the optimizer for the generator\n",
    "optimizer_dis = ... # setup the optimizer for the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = ...\n",
    "n_dis = ...  # Number of discriminator updates per generator update (more than 1 !)\n",
    "n_show = n_iter//10\n",
    "batch_size = ...\n",
    "\n",
    "gen_losses = []\n",
    "dis_losses = []\n",
    "\n",
    "pbar = tqdm(range(n_iter), desc=\"Training GAN\")\n",
    "\n",
    "x_labels = torch.ones((batch_size, 1))  # Real data labels (1 for real)\n",
    "y_labels = torch.zeros((batch_size, 1))  # Fake data labels (0 for fake)\n",
    "        \n",
    "for it in pbar :\n",
    "    for _ in range(n_dis):\n",
    "        x = ... # Sample real data\n",
    "        y = ...  # Sample from the generator\n",
    "        \n",
    "        # Train discriminator for a fixed generator (with y.detach())\n",
    "        optimizer_dis.zero_grad()\n",
    "        dis_loss_real = criterion(model.discriminate(x), x_labels)  # Loss for real data \n",
    "        dis_y = model.discriminate(y.detach())  # Detach to avoid backpropagation through generator\n",
    "        dis_loss_fake = criterion(dis_y, y_labels)  # Loss for fake data\n",
    "        dis_loss = (dis_loss_real + dis_loss_fake)/2  # Total discriminator loss\n",
    "        dis_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "        \n",
    "    dis_losses.append(dis_loss.item())\n",
    "    \n",
    "    # Train generator to fool the discriminator on a new batch\n",
    "    x = ... # Sample real data\n",
    "    y = ...  # Sample from the generator\n",
    "    \n",
    "    optimizer_gen.zero_grad()\n",
    "    dis_y = model.discriminate(y)\n",
    "    gen_loss = criterion(dis_y, x_labels)  # Generator loss using BCE with real labels\n",
    "    gen_loss.backward()\n",
    "    optimizer_gen.step()\n",
    "    \n",
    "    gen_losses.append(gen_loss.item())\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'gen loss'  : f\"{gen_losses[-1]:.4f}\",\n",
    "        'dis loss'  : f\"{dis_losses[-1]:.4f}\"\n",
    "    })\n",
    "    \n",
    "    if it % n_show == 0:\n",
    "        fig, axes = plot_GAN_critic(model)\n",
    "        axes[0].set_title(f\"GAN @ Iteration {it}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(gen_losses, label='GEN Loss', color='blue')\n",
    "ax.plot(dis_losses, label='DIS Loss', color='orange')\n",
    "ax.plot(np.log(2)/2*np.ones_like(dis_losses), '--', label='BCE for D=0.5', color='orange')\n",
    "ax.set_title('Losses during GAN Training')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss Value')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : Sampling the model and comparing to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = 1_000\n",
    "x = ...\n",
    "\n",
    "# Generate samples from the trained GAN\n",
    "y = ...\n",
    "\n",
    "# Define colors based on discriminator output\n",
    "\n",
    "x_label = model.discriminate(x).detach().numpy()\n",
    "y_label = model.discriminate(y).detach().numpy()\n",
    "\n",
    "x_colors = define_colors(x.numpy(), x_label= x_label)  \n",
    "y_colors = define_colors(y.numpy(), x_label= y_label)  \n",
    "fig, axes = plot_data_comparison(x,y, x_colors, y_colors)\n",
    "\n",
    "fig.suptitle(\"GAN generation of random samples\", fontsize=24)\n",
    "fig.colorbar(axes[0].collections[0], ax=axes[0], label='Data Probability', orientation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_GAN_critic(model)\n",
    "axes[0].set_title(\"*UNTRAINED* GAN Critic Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Discussion\n",
    "\n",
    "In this notebook, we have implemented a simple Generative Adversarial Network (GAN) using PyTorch. \n",
    "The generative model is composed of a decoder that maps a small latent space with a simple prior distribution to the (high-dimensional) data space.\n",
    "To train such a model, the GAN requires a discriminator that classifies the training and generatated data and which is trained jointly with the generator. \n",
    "\n",
    "Advantages and Drawbacks of GANs compared to other models:\n",
    "+ ✅ ...\n",
    "- ❌ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the next notebooks, we will explore more advanced generative models based on likelihood maximization that address some of these limitations, such as Normalizing Flow and Diffusion Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_demo_deep_gen_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
