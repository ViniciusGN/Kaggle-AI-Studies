{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab session #9 : Tiny GPT \n",
    "#### the goal of this notebook is to train a generative model on text data with a transformer architecture using causal self-attention. \n",
    "\n",
    "For simplicity, we will first use alpha-numeric characters as tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Ensicaen-logo.png\" alt=\"logo_ENSI\" style=\"width: 200px;\"/> \n",
    "\n",
    "### 2023 Notebook by Julien (dot) Rabin (at) ensicaen.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### LastName / Nom : \n",
    "### Surname / Prénom : \n",
    "### Group :\n",
    "### Date : \n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today's Menu\n",
    "\n",
    "In this notebook, the goal is to train a generative model from a text-dataset following these steps :\n",
    "\n",
    "- [Useful Torch libraries](#0---load-libraries-and-fetch-data) : load necessary libraries and fetch a text dataset\n",
    "- [Data - Preprocessing](#1---data-pre-processing) define a trivial tokenizer using lookup table on characters\n",
    "- [Toy model](#2---bi-gram-generative-model) A complete but very shallow to-model to predict next token based only on the previous one\n",
    "\n",
    "    -[Batch routine trick for parallelized training](#21---definine-torch-routine-to-process-data-enconder---decoder)\n",
    "\n",
    "    -[Bi-Gram model](#22---definine-bigram-model)\n",
    "\n",
    "    -[Test Random Model :](#23---test-non-trained-model)\n",
    "\n",
    "    -[Model Training :](#24---train-model)\n",
    "\n",
    "    -[Model generation :](#25---sampling-the-generative-model)\n",
    "    \n",
    "- [Transformer Model](#3---using-pytorch-transformer-encoder-model) model based on a transformer Encoder trained with causal masked attention \n",
    "\n",
    "    -[Causal attention masks](#32---causal-attention-with-masked-inputs)\n",
    "\n",
    "    -[Enhanced Model with Tranformer Encoder](#33---define-full-generative-model-with-transformers)\n",
    "\n",
    "    -[Training the transformer model](#34---train-the-transformer-model)\n",
    "\n",
    "    -[Evaluating the generative model](#35---evaluating-the-model)\n",
    "\n",
    "- [Questions](#4---questions) some simple questions that should be addressed within the lab session\n",
    "- [Exercices](#5---exercices) pick a few questions from the proposed exercice to deeppen your understanding of the method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - load libraries and fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# for Jupyter notebook\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select / create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = './julesverne_tourdumonde80jours_UTF8.txt' # converted into UTF8 to deal with french superb accents\n",
    "\n",
    "print(\"lien vers catalogue des Textes du CNAM (~300 livres) : http://abu.cnam.fr/BIB/\")\n",
    "print(\"(voir ReadMe.txt pour conversion UTF8)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(input_file_path, encoding=\"utf-8\", mode=\"r\")\n",
    "data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - data pre-processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some data characters\n",
    "n = 20_000\n",
    "for k in range(10) :\n",
    "    print(data[n+k*100:n+(k+1)*100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = int(.9 * len(data)) # percentage of data for training\n",
    "data_train = data[:p]\n",
    "data_eval  = data[p:]\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = set(data_train) # decompose the data stream into a set of unique characters\n",
    "token_list = sorted(token_list) # lexicographic sorting\n",
    "print(token_list)\n",
    "\n",
    "token_size = len(token_list)\n",
    "print(\"Number of Tokens :\",token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lookup tables\n",
    "char2token = {val:idx for idx,val in enumerate(token_list)}\n",
    "print(char2token)\n",
    "token2char = {idx:val for idx,val in enumerate(token_list)}\n",
    "print(token2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encoder = lambda x : [char2token[xi] for xi in x]\n",
    "token_decoder = lambda t : [token2char[ti] for ti in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = token_encoder(data_train)\n",
    "print(\"Encoding :\", encoded_data_train[n:n+100])\n",
    "decoded_data_train = token_decoder(encoded_data_train)\n",
    "decoded_data_train = ''.join(decoded_data_train) # convert list of char to string\n",
    "print('____')\n",
    "print(\"Decoding :\",  decoded_data_train[n:n+100])\n",
    "\n",
    "print('____')\n",
    "print(\" > Sanity check : is data = decoder (encoder(data)) ?\", decoded_data_train == data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rince & repeat for evaluation data data_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test with random tokens :')\n",
    "x = np.random.randint(0,token_size,(100,))\n",
    "print(\"input :\", x)\n",
    "print(\"output :\", token_decoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Toy-Model : (non-causal) Bi-Gram Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram model consists in predicting a token $t_k$ at position $k$ given a context of the previous $n-1$ tokens $(t_{k-1}, ... t_{k-n+1})$.\n",
    "\n",
    "```Note```: for images where token are pixels (e.g. pixel RNN/CNN), the context is equivalent to consider the patch of the neighboring pixels.\n",
    "\n",
    "Formally, we want to train a generative model parametrized by $\\theta$  which \n",
    "1) predicts given a context $(t_{k-1}, ... t_{k-n+1})$ the most likely token values $x$ for $t_k$, that is to learn the conditional probability on the observed data\n",
    "$$\n",
    "    \\forall x \\in C, \\; \\text{Pr}(t_k = x | t_{k-1}, ... t_{k-n+1})\n",
    "$$\n",
    "\n",
    "```Note```: in practice, the context can also include future tokens for training purposes only (non-causal model like BERT) or for different task (classification).\n",
    "\n",
    "2) samples from the multinomial \n",
    "\n",
    "For now, lets start with $n=2$\n",
    "\n",
    "some ref : https://fr.wikipedia.org/wiki/N-gramme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - DataLoader routine \n",
    "torch routines to train in parallel strings of tokens during training and process text during generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(encoded_data = encoded_data_train, context_size = 8, batch_size = 4) :\n",
    "\n",
    "    idx = np.random.randint(0,len(encoded_data) - context_size, (batch_size))\n",
    "    x = [ torch.tensor(encoded_data[i:i+context_size]) for i in idx ]\n",
    "    x = torch.stack(x, dim=0) # [batch_size x context_size]\n",
    "\n",
    "    # right to left shifting :\n",
    "    y = [ torch.tensor(encoded_data[i+1:i+1+context_size]) for i in idx ] # y is x shifted right and use to define target prediction\n",
    "    y = torch.stack(y, dim=0) # [batch_size x context_size]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch(encoded_data_train, context_size = 64, batch_size = 1)\n",
    "print(x.shape)\n",
    "print(f\"x={x.numpy()} and\\ny={y.numpy()}\")\n",
    "x = token_decoder(x[0].numpy())\n",
    "y = token_decoder(y[0].numpy())\n",
    "print(f\"decoding :\\nx: {''.join(x)} \\ny: {''.join(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui decode directement un tenseur (batch x N) en une liste de batch string de N caractères\n",
    "def batchtok_to_strlist(x) :\n",
    "    s = []\n",
    "    for b in range(x.size(0)) :\n",
    "        l = token_decoder(x[b].numpy().tolist())\n",
    "        s.append(''.join(l))\n",
    "    return s\n",
    "\n",
    "if False : # random token stream\n",
    "    x = torch.randint(token_size, (2,128))\n",
    "else : # real token stream\n",
    "    x,_ = get_batch(encoded_data_train, context_size = 64, batch_size = 2)\n",
    "print(x)\n",
    "\n",
    "s = batchtok_to_strlist(x)\n",
    "print(s)\n",
    "print(*s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - definition of a bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete the following toy model and answer the questions\n",
    "1. Read & Complete this notebook, starting with very small datasets (for instance, changing the data split between `data_train` & `data_eval`):\n",
    "    1. complete the initialisation of the toy model for positional embedding (e.g. constant -i.e. no embedding-, using index position and `nn.embedding` torch layer, high frequency cosine function, random vectors  ...)\n",
    "    2. complete the forward model with the definition of the loss function using cross entropy\n",
    "    3. complete the generation method to generate `gen_size` tokens rather than only one, at a given `temperature`\n",
    "    4. complete the training algorithm (optimiser, auto-diff loss derivation) and display aggregated cross entropy loss on both the complete training dataset (say for each epoch)\n",
    "    5. what is the role of the temperature parameter during generation ?\n",
    "2. assess the overfitting of the model using evaluation data by comparising train/eval loss\n",
    "    1. train with different dataset split ratio (e.g. 80/20, 90/10, 95/5)\n",
    "    2. train with different model capacity (e.g. embedding dimension, number of layers in MLP)\n",
    "3. assess the quality of the generated text sequences by varying the temperature parameter during generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Answer the following questions :\n",
    "- Why do we need embeddings for tokens ?\n",
    "- Why do we need embeddings for positions ?\n",
    "- Why to we use the `CrossEntropyLoss` for training *without* `Softmax` layer at the end of the classification network ?\n",
    "- Why do we need a `Softmax` layer during generation ?\n",
    "- What is the role of the MLP (multi-layer perceptron) here ? is it the same for every processed token ?\n",
    "- Can tokens communicate with each other in this model ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_model_class (nn.Module) :\n",
    "    def __init__(self,embed_dim=32, context_size=64):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = 4*embed_dim\n",
    "\n",
    "        # position embedding, for instance use 0 for token 0, ... i for token i, etc \n",
    "        self.pos = ... # Note: if =0, use the same positional tokens (0) at every position\n",
    "        self.pos_embedding = nn.Embedding(context_size,embed_dim).to(device) # position to vector representation\n",
    "\n",
    "        self.token_embedding = nn.Embedding(token_size,embed_dim).to(device) # token to vector representation\n",
    "        \n",
    "        # MLP\n",
    "        self.linear1 = nn.Linear(embed_dim, self.hidden_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # Linear classification layer\n",
    "        self.last_linear = nn.Linear(self.embed_dim, token_size)\n",
    "\n",
    "    def forward(self,inputs, targets = None):\n",
    "        B,T = inputs.size()\n",
    "        assert T == self.context_size\n",
    "\n",
    "        # inputs & targets are token batch tensor whose shape is [B x T] = [batch_size x self.context_size]\n",
    "        tok = self.token_embedding(inputs) # [B x T x D] where D = self.embed_dim\n",
    "        pos = self.pos_embedding(self.pos) # [1 x T x D]\n",
    "        inputs = tok + pos # [B x T x D]\n",
    "        \n",
    "        # residual block\n",
    "        x = self.dropout(self.activation(self.linear1(inputs)))\n",
    "        x = self.dropout(self.activation(self.linear2(x)))\n",
    "        inputs = inputs + self.layer_norm(x)\n",
    "\n",
    "        logits = self.last_linear(inputs) # [B x T x N] where N = token_size\n",
    "        \n",
    "        if targets is not None : # compute the loss function\n",
    "            loss = ... # using CrossEntropyLoss. Warning some reshaping required to get a tensor : [Batch x #Class]\n",
    "        else : # during generation, no need to compute the loss function\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, gen_size = 1, temperature = 1.) :\n",
    "        B,T = inputs.size()\n",
    "        if T < self.context_size : # padding with 0 or 1 tokens (= '\\n' or ' ') \n",
    "            inputs = torch.cat((torch.zeros((B,self.context_size - T), dtype=torch.long), inputs), dim=1) # [B x context_size]\n",
    "            x = inputs\n",
    "        else : # truncation with the maximum context\n",
    "            x = inputs[:,:self.context_size] \n",
    "        \n",
    "        logits, _ = self(x) # forward pass\n",
    "        logits = logits[:,-1,:] # [B x 1 x N] as only the last predicted token is useful\n",
    "        \n",
    "        p = ... # convert logits to probabilities using softmax and temperature [B x 1 x N]\n",
    "        \n",
    "        tok = torch.multinomial(p, 1) # random token based on the predicted probabilities\n",
    "        inputs = torch.cat((inputs, tok), dim=1) # add the generated token to the end of the token-stream\n",
    "\n",
    "        return inputs[:,context_size:] # [B x gen_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_num_param(model) :\n",
    "    N = 0\n",
    "    for name, param in model.named_parameters() :\n",
    "        print(name)\n",
    "        N += param.view(-1).size(0)\n",
    "    print(\"Number of model parameter :\",N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - test non-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random model\n",
    "embed_dim = 16 # dimension of embedding for token (and position)\n",
    "context_size = 32 # lenght of token sequences in the batch\n",
    "\n",
    "bigram_model = ... # do not forget to use device !\n",
    "\n",
    "print(bigram_model)\n",
    "print_model_num_param(bigram_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loss for training data\n",
    "batch_size = 100\n",
    "x,y = get_batch(encoded_data = encoded_data_train, context_size = context_size, batch_size = batch_size)\n",
    "x,y = x.to(device), y.to(device)\n",
    "\n",
    "bigram_model.eval() # removes dropout and other normalization\n",
    "logits,loss = ...\n",
    "bigram_model.train()\n",
    "\n",
    "print(\"cross entropy loss\", loss.detach().cpu())\n",
    "print(\"expected value for uniform random\", np.log(token_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prediction for random data\n",
    "batch_size = 4\n",
    "x = torch.randint(token_size, (batch_size,context_size)).to(device)\n",
    "strlist = batchtok_to_strlist(x.cpu())\n",
    "print(\"random stream of char : \", strlist)\n",
    "\n",
    "bigram_model.eval()\n",
    "y,_ = ... # using forward mode w/o targets : logits for each position are returned \n",
    "bigram_model.train()\n",
    "\n",
    "y = ... # extract predicted logits for the last position\n",
    "p = ... # convertion to probability\n",
    "tok = ... # select the most predictible token\n",
    "print(\"most probable token /char is {} = '{}'\".format(tok, batchtok_to_strlist(tok.cpu())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation\n",
    "x,_ = get_batch(encoded_data_train, batch_size=1, context_size=context_size)\n",
    "x = x.to(device)\n",
    "\n",
    "bigram_model.eval()\n",
    "y = ... # generate a string of 100 tokens\n",
    "bigram_model.train()\n",
    "\n",
    "print(\"input tokens\", x)\n",
    "print(\"input str\", batchtok_to_strlist(x.cpu()))\n",
    "print(\"output tokens\", y)\n",
    "synth = batchtok_to_strlist(y.cpu())\n",
    "print(\"output str =\\n\", synth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_model)\n",
    "N = 0\n",
    "for name, param in bigram_model.named_parameters() :\n",
    "    print(name)\n",
    "    N += param.view(-1).size(0)\n",
    "print(\"Number of model parameter :\",N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_bigram = torch.optim ...\n",
    "batch_size = 128\n",
    "\n",
    "nepoch = 1\n",
    "niter = int(nepoch * len(encoded_data_train) / batch_size)‡\n",
    "print(\"niter = \", niter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = []\n",
    "for it in range(niter) :\n",
    "    x,y = get_batch(encoded_data_train, context_size=bigram_model.context_size, batch_size=batch_size)\n",
    "    x,y = x.to(device),y.to(device)\n",
    "\n",
    "    loss = ...\n",
    "    \n",
    "    optim_bigram.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_bigram.step()\n",
    "\n",
    "    Loss.append(loss.item())\n",
    "\n",
    "    if (it % int(niter//100)) == 0 :\n",
    "        print(\"it = %d / %d : loss = %f\" % (it, niter, Loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Loss)\n",
    "plt.title('Loss on training batch evaluations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 -  sampling the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation\n",
    "x,_ = get_batch(encoded_data_train, batch_size=1, context_size=bigram_model.context_size)\n",
    "x = x.to(device)\n",
    "y = ... # e.g with gen_size = 100, temperature = 1.\n",
    "#print(\"input tokens\", x)\n",
    "print(\"input str\", batchtok_to_strlist(x.cpu()))\n",
    "#print(\"output tokens\", y)\n",
    "print(\"output str\", batchtok_to_strlist(y.cpu())) # generated string with default temperature = 1. used during training\n",
    "y = ... # now with gen_size = 100, temperature = 10.)\n",
    "print(\"with high temperature\", batchtok_to_strlist(y.cpu())) # what happens now ?\n",
    "y = ... # now with gen_size = 100, temperature = 0.1)\n",
    "print(\"with low temperature\", batchtok_to_strlist(y.cpu())) # what happens now ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion on the bigram model : \n",
    "- What is the role of the `temperature` parameter  ?\n",
    "- what are the limitations of using such a bigram model ? For instance, why sequences like \"nnn\" are generated so often ?\n",
    "- How could the model be improved to generate more coherent sequences ?\n",
    "- How can we create a N-gram model with N>2 in this framework ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________\n",
    "# 3 - Generative model based on pytorch transformer model\n",
    "\n",
    "Recall that GPT is a based on the encoder-decoder transformer architecture proposed in \n",
    "\"attention is all you need\" 2017 Vaswani et al. \n",
    "This general architecture is composed of two main parts:\n",
    "- The encoder uses self-attention on the **full sequence** to compute a *latent representation of the input sequence*. \n",
    "- The decoder takes as inputs **both the input sequence and this latent representation**, making use of causal self- and cross-attention between representation. Causality is ensured in the decoder by masking future tokens.\n",
    "\n",
    "Here, since we want to build a generative model, we will only use the decoder part of the transformer architecture with **causal self-attention**.\n",
    "Therefore, since we do not need for a latent representation of the full sequence, we will use the `nn.TransformerEncoder` module from pytorch.\n",
    "\n",
    "##### Exercice\n",
    "complete the following cells exploring the encoder architecture, and the definition of the causal attention masks. Then complete the proposed `transformer_model_class` to build a full generative model based on transformer encoder with causal attention. Last, train the model and evaluate its performance on text generation. \n",
    "\n",
    "**Warning !** To start with, use a very small dataset (e.g. 1000 characters) and a small model (e.g. embedding dimension 16, 2 layers with 2 heads) to check that everything works fine before increasing the model capacity and dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - test with torch transformer encoder `torch.nn.TransformerEncoder`\n",
    "\n",
    "Warning ! by default, `batch_first` is set to `False` in torch transformer models. \n",
    "So the input shape is $[T \\times B \\times d]$, that is (seq_len, batch_size, embed_dim)\n",
    "\n",
    "\n",
    "<!--- the following code include figures from repertory /fig: transformer_architecture.jpg and transformer_block.jpg -->\n",
    "![Transformer architecture](fig/transformer_block.jpg)\n",
    "![Transformer architecture](fig/transformer_architecture.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with torch.nn.TransformerEncoder\n",
    "# modèle : torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None, enable_nested_tensor=True, mask_check=True)\n",
    "# forward : forward(src, mask=None, src_key_padding_mask=None, is_causal=None)\n",
    "\n",
    "embed_dim = ... # dimension of each token embedding\n",
    "num_heads = 1 # number of heads in attention model\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads).to(device)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).to(device)\n",
    "\n",
    "# /!\\ batch dimension is 1 rather than 0  \n",
    "inputs = torch.rand((context_size, batch_size, transformer_model.d_model), device=device) # [context_size x  batch x feature_dim] : src should be encoded (with pos embedding) before fed to the encoder input\n",
    "print(\"inputs = \",inputs)\n",
    "outputs = transformer_model.encoder(inputs)\n",
    "print(\"outputs = \",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_encoder)\n",
    "print_model_num_param(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on true data, with mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - causal attention with masked inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an additive mask\n",
    "x,_ = get_batch(encoded_data_train, batch_size=batch_size, context_size=context_size) # B x T\n",
    "x = x.to(device)\n",
    "inputs = tok2vec_emb(x) # [B x T x d]\n",
    "inputs = inputs.permute(1,0,2) # [T x B x d]\n",
    " \n",
    "# mask has to be [T x T]\n",
    "# mask[i,j] = 0 if i<= j else inf\n",
    "mask = torch.tril(torch.ones(context_size,context_size), diagonal=0)\n",
    "\n",
    "mask[mask<1.] = -np.inf # the dot product is equal to -inf for future (unseen) token\n",
    "mask[mask>=1.] = 0.\n",
    "mask = mask.to(device)\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test the model, using the provided mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - a full generative model with transformer encoder blocks\n",
    "\n",
    "- same as previous bigram class model where we basically replace the MLP with several masked transformer encoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_model_class (nn.Module) :\n",
    "    def __init__(self,embed_dim=32, context_size=64, num_heads=8, num_layers=1, dim_feedforward=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert (embed_dim % num_heads == 0) # embed_dim must be multiple of num_heads\n",
    "\n",
    "        self.pos = ... # precomputed positional token\n",
    "        self.pos_emb = nn.Embedding(context_size,embed_dim) # position to vector representation\n",
    "\n",
    "        self.tok2vec_emb = nn.Embedding(token_size,embed_dim).to(device) # token to vector representation\n",
    "        self.vec2tok_emb = nn.Linear(embed_dim, token_size).to(device) # vector to token logits\n",
    "        \n",
    "        # use here a transformer encoder\n",
    "        encoder_layer = ...\n",
    "        self.transformer_encoder = ...\n",
    "        \n",
    "        # causal mask\n",
    "        mask = torch.tril(torch.ones(context_size,context_size), diagonal=0)\n",
    "        mask[mask<1.] = -np.inf # the dot product is equal to -inf for future (unseen) tokens\n",
    "        mask[mask>=1.] = 0.\n",
    "        self.mask = mask.to(device)\n",
    "\n",
    "    def forward(self,inputs, targets = None):\n",
    "        B,T = inputs.size()\n",
    "        assert T == self.context_size\n",
    "        \n",
    "        # inputs & targets are token batch tensor whose shape is [B x T] = [batch_size x self.context_size]\n",
    "        tok = self.tok2vec_emb(inputs) # [B x T x D] where D = self.embed_dim\n",
    "        pos = self.pos_emb(self.pos) # [1 x T x D]\n",
    "        tok = tok + pos # [B x T x D]\n",
    "        inputs = tok.permute(1,0,2) # [T x B x D]\n",
    "        outputs = self.transformer_encoder(inputs, self.mask) # [T x B x D]\n",
    "        outputs = outputs.permute(1,0,2) # [B x T x D]\n",
    "        logits = self.vec2tok_emb(outputs) # [B x T x N] where N = token_size\n",
    "        \n",
    "        if targets is not None :\n",
    "            loss = ...\n",
    "        else :\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, gen_size = 1, temperature = 1.) :\n",
    "        \n",
    "        B,T = inputs.size()\n",
    "        if T < self.context_size : # padding with 0 or 1 tokens (= '\\n' or ' ') \n",
    "            inputs = torch.cat((torch.zeros((B,self.context_size - T), dtype=torch.long, device=device), inputs), dim=1)\n",
    "        \n",
    "        x = inputs[:,-self.context_size:] if (inputs.size(1) > self.context_size) else inputs # truncation of maximum context\n",
    "        \n",
    "        logits, _ = self(x) # forward pass\n",
    "        logits = logits[:,-1,:] # [B x N] as only the last predicted token is useful\n",
    "        \n",
    "        p = ... # logits to probabilities [B x N], using temperature \n",
    "        \n",
    "        tok = torch.multinomial(p, 1) # [B] random token based on the predicted probabilities\n",
    "        inputs = torch.cat((inputs, tok), dim=1) # add the generated token to the end of the token-stream\n",
    "\n",
    "        inputs = inputs[:,self.context_size:]\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "num_heads = 2 # number of heads in attention model \n",
    "embed_dim = 4*num_heads # note : \"each head will have dimension embed_dim // num_heads\"\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "dim_feedforward = 4*embed_dim\n",
    "\n",
    "# data parameter\n",
    "context_size = 32 # input size of the transformer encoder\n",
    "\n",
    "transformer_model = ... # remember to use 'device' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(transformer_model)\n",
    "N = 0\n",
    "for name, param in transformer_model.named_parameters() :\n",
    "    #print(name)\n",
    "    #print(param.shape)\n",
    "    N += param.view(-1).size(0)\n",
    "print(\"Number of model parameter :\",N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forward model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the (untrained) generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - train the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_transformer = torch.optim...\n",
    "batch_size = 16\n",
    "\n",
    "Loss = []\n",
    "\n",
    "nepoch = 10\n",
    "niter = int(nepoch * len(encoded_data_train) / batch_size)\n",
    "#niter = 5000\n",
    "print(\"niter = \", niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.train()\n",
    "for it in range(niter) :\n",
    "    x,y = get_batch(encoded_data_train, context_size=transformer_model.context_size, batch_size=batch_size)\n",
    "    x,y = x.to(device), y.to(device)\n",
    "\n",
    "    loss =  ...\n",
    "    \n",
    "    optim_transformer.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_transformer.step()\n",
    "\n",
    "    Loss.append(loss.item())\n",
    "\n",
    "    if (it % int(niter//100)) == 0 :\n",
    "        print(\"/!\\ calculer le critère d'évaluation sur les ensembles data_train / data_eval\")\n",
    "        print(\"it = %d / %d : loss = %f\" % (it, niter, Loss[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - evaluating the model\n",
    "test the trained generative model on train data, eval data, and generate new tokens based on your own sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Proposed extensions\n",
    "pick a few questions from the proposed exercice to deeppen your understanding of the method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suggestion 1 :\n",
    "- experiment with other datasets, for instance python code (Mostly Basic Python Problems Dataset : https://github.com/google-research/google-research/tree/master/mbpp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suggestion 2 : using a more powerful tokenizer\n",
    "\n",
    "For instance, use the GPT2 BPE tokenizer and train your model on english data (using public domain data, e.g. https://www.gutenberg.org/cache/epub/63355/pg63355.txt)\n",
    "\n",
    "You can take inspiration from the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data : Pasteur book in english\n",
    "data_url = \"https://www.gutenberg.org/cache/epub/63355/pg63355.txt\"\n",
    "!wget $data_url -O data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt')\n",
    "train_data = f.read()\n",
    "f.close()\n",
    "\n",
    "print(\"Number of characters\", len(train_data))\n",
    "\n",
    "n = 10_000\n",
    "print(\" some excerpt : \", train_data[n:n+1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_tok = enc.encode_ordinary(train_data)\n",
    "token_size = max(train_tok)\n",
    "print(f\"dataset has been split in {len(train_tok):,} tokens from {min(train_tok)} to {token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = train_tok[n:n+100]\n",
    "print(\"excerpt :\", seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ori = train_data[n:n+100]\n",
    "print(\"\\t original :\\n\" + seq_ori)\n",
    "print(\"_\" * 50)\n",
    "# decoding\n",
    "seq_char = enc.decode(enc.encode(seq_ori))\n",
    "print(\"\\tdecoded :\\n\" + seq_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
