{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4QcKu0nCEit"
      },
      "source": [
        "# PageRank as an optimization problem\n",
        "\n",
        "The goal of this notebook is to implement Stochastic Gradient Descent (SGD) and compare it to Gradient Descent (GD) for solving the PageRank optimization problem\n",
        "$$\n",
        "\\min_{x \\in \\mathbb{R}^n} f(x) := \\frac{1}{2}\\Vert Mx - x\\Vert^2 + \\frac{\\lambda}{2} \\left(\\sum_{i=1}^n x[i] - 1\\right)^2 ,\n",
        "$$\n",
        "where $M = (1 - \\alpha)A + \\alpha/n$, $\\alpha \\in ]0,1]$ ($0.15$ in our implementation), and $A$ is the link matrix of a web. The theoretical properties of GD and SGD for quadratic problems were studied in the course and TD 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pubOB18CEiv"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "import os\n",
        "import sys\n",
        "import curses\n",
        "import numpy as np\n",
        "import numpy.matlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import scipy.sparse as scisparse\n",
        "import pkgutil\n",
        "from time import time\n",
        "from importlib import reload\n",
        "from matplotlib import interactive\n",
        "interactive(True)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnPAjNBSCEiw"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfPKsRjWCEiw"
      },
      "outputs": [],
      "source": [
        "def mMatrix(A,alpha = 0.15):\n",
        "    ''' Creates the regularized version of A with alpha=0.15 '''\n",
        "    n = A.shape[0]\n",
        "    M = (1-alpha)*A+alpha/n\n",
        "    return M\n",
        "\n",
        "def scorePageRank(A):\n",
        "    ''' Computes the score vector using a full eigenvalue decomposition '''\n",
        "    n = A.shape[0]\n",
        "    M = mMatrix(A)\n",
        "    v, w = np.linalg.eig(M)\n",
        "    i = np.where(np.abs(v-1) < 1e-5)\n",
        "    score = w[:, i].reshape((n, 1))\n",
        "    score /= score.sum()\n",
        "    return score.real\n",
        "\n",
        "def dotProd(x, y):\n",
        "    ''' Computes the dot product between x and y '''\n",
        "    return np.ndarray.item(np.inner(x.reshape(-1), y.reshape(-1)))\n",
        "\n",
        "def MakeCol(y): return y.reshape(-1,1)\n",
        "\n",
        "def computeEigHF(A, lam):\n",
        "    ''' Computes the eigenvalue decomposition of the Hessian to get the convergence rate of GD with step-size gamma.\n",
        "    Parameters:\n",
        "    -----------\n",
        "        A (np.matrix): the matrix of links\n",
        "        lam (float): regularization parameter\n",
        "    Returns:\n",
        "    --------\n",
        "        v (float): the eignevalues of HF\n",
        "    '''\n",
        "    n = A.shape[0]\n",
        "    M = mMatrix(A)\n",
        "    In = np.eye(n)\n",
        "    e = np.matlib.ones((n, 1))\n",
        "    HF = (In-M).transpose()*(In-M) + lam*e*e.transpose()\n",
        "    v, w= np.linalg.eig(HF)\n",
        "    return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzU8ygn6CEix"
      },
      "source": [
        "# Miniwebs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXuMh5yRCEix"
      },
      "outputs": [],
      "source": [
        "def createMiniweb1():\n",
        "    ''' Creates a 4x4 miniweb matrix '''\n",
        "    A = np.matrix([[0, 1/3., 1/3., 1/3.],\n",
        "                   [0, 0, 1/2., 1/2.],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [1/2., 0, 1/2., 0]]).transpose()\n",
        "    return A\n",
        "\n",
        "\n",
        "def createMiniweb2():\n",
        "    ''' Creates a 5x5 miniweb matrix '''\n",
        "    A = np.matrix([[0, 1, 0, 0, 0],\n",
        "                   [1, 0, 0, 0, 0],\n",
        "                   [0, 0, 0, 1, 0],\n",
        "                   [0, 0, 1, 0, 0],\n",
        "                   [0., 0, 1/2., 1/2., 0]]).transpose()\n",
        "    return A\n",
        "\n",
        "\n",
        "def createRandomWeb(n=15, density=0.15, sparse=True):\n",
        "    ''' Creates a random web link matrix\n",
        "    Parameters:\n",
        "    -----------\n",
        "        n (int, optional): size of the matrix\n",
        "        density (float, optional) : density rate, should lie in [0,1]\n",
        "        sparse (boolean, optional): True (sparse) False (full)\n",
        "    Returns:\n",
        "    --------\n",
        "        A (np.matrix): the randomly generated link matrix\n",
        "    Notes:\n",
        "    ------\n",
        "        The routine starts by choosing random connections, then add other\n",
        "        random connections if some pages are not connected to any other, and\n",
        "        then normalizes to compute the column-stochastic link matrix.\n",
        "    '''\n",
        "    A = np.matlib.zeros((n*n, 1))\n",
        "    indices = np.random.choice(range(n*n), int(density*n*n), replace=False)\n",
        "    A[indices] = 1\n",
        "    A = A.reshape((n, n))\n",
        "    nLinks = np.sum(A, 0)\n",
        "    nonConnectedCols = np.arange(n)[np.asarray(nLinks).reshape(-1) == 0]\n",
        "    nLinks = np.asarray(np.matlib.repmat(nLinks, n, 1))\n",
        "    indices = np.random.choice(range(n), len(nonConnectedCols), replace=True)\n",
        "    A[A != 0] = 1 / nLinks[A != 0]\n",
        "    A[indices, nonConnectedCols] = 1\n",
        "    if sparse is True:\n",
        "        return scisparse.csr_matrix(A)\n",
        "    else:\n",
        "        return A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBM-NE7lCEix"
      },
      "source": [
        "# Gradient descent\n",
        "Apply GD\n",
        "$$\n",
        "x_{k+1} = x_k - \\gamma \\nabla f(x_k)\n",
        "$$\n",
        "to solve the PageRank optimization problem. The gradient $\\nabla f(x)$ must be computed efficiently as\n",
        "$$\n",
        "\\nabla f(x) = x-z - (1-\\alpha)A^\\top(x-z) + \\lambda(s-1)\n",
        "$$\n",
        "where $s=\\sum_{i=1}^n x[i]$ and $z=(1-\\alpha)Ax + \\alpha s/n$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this `def computeEigHF(A, lam):` perform this calculus that is used to see the convergence\n",
        "`B = ((I - M).T)*(I - M) + lambda * s * s.T`\n",
        "\n",
        "gradient = (M - I).T * (M - I)x + lambda*((s.T*x -1))"
      ],
      "metadata": {
        "id": "TLPfIwniLn4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMIsftcCEiy",
        "outputId": "497952f8-8d76-4fdd-ea59-24d56dd048f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33290087]\n",
            " [0.17342027]\n",
            " [0.2690128 ]\n",
            " [0.21064919]]\n",
            "[[0.36815068]\n",
            " [0.14180936]\n",
            " [0.28796163]\n",
            " [0.20207834]]\n"
          ]
        }
      ],
      "source": [
        "my_miniweb = createMiniweb1()\n",
        "M = mMatrix(my_miniweb)\n",
        "# L = M - np.eye(M.shape[0])  # L = M-I for beta\n",
        "\n",
        "def f(x, lam):\n",
        "  first_term = 0.5*(np.linalg.norm((M @ x)-x)**2)\n",
        "  second_term = (lam/2)*(np.sum(x)-1)**2\n",
        "  fx = first_term + second_term\n",
        "  return fx\n",
        "\n",
        "def gradient(x, A, lam, alpha=0.15):\n",
        "  z = (1-alpha)*A @ x + (alpha*np.sum(x)/x.shape[0]) * np.ones((x.shape[0], 1))\n",
        "  grad = x - z - (1-alpha)*A.T @ (x-z) + lam*(np.sum(x)-1) * np.ones((x.shape[0], 1))\n",
        "  return grad\n",
        "\n",
        "def grad_step(xk, lr=0.001):\n",
        "  return xk - lr*gradient(xk, my_miniweb, 1)\n",
        "\n",
        "x_zero = np.ones((my_miniweb.shape[0], 1))\n",
        "x_zero = x_zero/np.sum(x_zero)\n",
        "xk_plus = x_zero\n",
        "n_iter=1000\n",
        "\n",
        "for _ in range(n_iter):\n",
        "  xk_plus = grad_step(xk_plus)\n",
        "\n",
        "print(xk_plus)\n",
        "print(scorePageRank(my_miniweb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyZTJn2-CEiy"
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "The PageRank optimization objective can be equivalently written as\n",
        "$$\n",
        "f(x) = \\frac{1}{2(n+1)}\\Vert \\mathcal{A} x - b\\Vert^2 .\n",
        "$$\n",
        "where $\\mathcal{A}=\\begin{pmatrix} I - M \\\\ \\sqrt{\\lambda} E \\end{pmatrix}$ and $b=(0,\\cdots,0,\\sqrt{\\lambda})^\\top$, $E=(1,\\ldots,1)$. Computing $\\nabla f(x)$ is prohibitive for large $k$ (cost/iter is $O(n^2)$ for dense and $O(density*n^2)$ for sparse graph). This finite sum structure (empirical risk) is amenable to SGD\n",
        "$$\n",
        "x_{k+1} = x_k - \\gamma \\nabla f_{i_k}(x_k)\n",
        "$$\n",
        "where $i_k$ is drawn uniformly at random in $\\{1,\\ldots,n+1\\}$, and\n",
        "$$\n",
        "\\nabla f_i(x) =\n",
        "\\begin{cases}\n",
        "\\left((1-\\alpha)a_i +  \\frac{\\alpha}{n} - e_i\\right) \\left((1-\\alpha)\\langle a_i,x \\rangle + \\frac{\\alpha}{n} \\sum_{j=1}^n x[j] - x[i]\\right) & i=1,\\ldots,n , \\\\\n",
        "\\lambda \\left(\\sum_{j=1}^n x[j] - 1\\right) & i=n+1 ,\n",
        "\\end{cases}\n",
        "$$\n",
        "with $a_i = A[i,:]^\\top$. Computing $\\nabla f_i(x)$ costs $O(n)$ per iteration. See also the comparison of running times is the last section of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1mbApirCEiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ceb38c-aa3e-44cb-c3b9-c03893c99b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.35792968]\n",
            " [0.15186113]\n",
            " [0.28123494]\n",
            " [0.20507358]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Insert your code here.\n",
        "from numpy import linalg as LA\n",
        "\n",
        "def residual_error(x, x_star):\n",
        "  return LA.norm(x - x_star)\n",
        "\n",
        "x_zero = np.ones((my_miniweb.shape[0], 1))\n",
        "x_zero = x_zero/np.sum(x_zero)\n",
        "sgd_xk_plus = x_zero.copy()\n",
        "n_iter=1000\n",
        "n = my_miniweb.shape[0]\n",
        "res_error = []\n",
        "x_star = scorePageRank(my_miniweb)\n",
        "\n",
        "def f(x, n, lam):\n",
        "  A_1 = np.eye(M.shape[0]) - M  # L = M-I for beta\n",
        "  A_2 = np.sqrt(lam) * np.ones((1, n))  # (1, n) - vetor linha\n",
        "  capital_A = np.vstack((A_1, A_2))\n",
        "  b = np.zeros(n+1)\n",
        "  b[-1] = np.sqrt(lam)\n",
        "\n",
        "  first_term = 1 / (2 * (n + 1))\n",
        "  second_term = LA.norm(capital_A @ x - b)**2\n",
        "\n",
        "  return first_term * second_term\n",
        "\n",
        "def gradient(idx, x, A, lam, alpha=0.15):\n",
        "  if idx < n:\n",
        "    alpha_n = alpha/n\n",
        "\n",
        "    e_i = np.zeros((n, 1))\n",
        "    e_i[idx] = 1\n",
        "\n",
        "    first_term = ((1-alpha)*A[idx,:].T + (alpha_n) - e_i)\n",
        "    second_term = (1-alpha)*(dotProd(A[idx,:].T,x)) + alpha_n * np.sum(x) - x[idx]\n",
        "\n",
        "    result = first_term * second_term\n",
        "  else:\n",
        "    result = lam*(np.sum(x)-1)\n",
        "  return result\n",
        "\n",
        "def grad_step(xk, idx, lr=0.01):\n",
        "  # x_{k+1} = x_k - \\gamma \\nabla f_{i_k}(x_k)\n",
        "  return xk - lr*gradient(idx=idx, x=xk, A=my_miniweb, lam=1, alpha=0.15)\n",
        "\n",
        "def SGD(n_iter=1000, step_size=0.001)\n",
        "  for _ in range(n_iter):\n",
        "    idx_k = np.random.randint(0, n + 1)\n",
        "    sgd_xk_plus = grad_step(xk=sgd_xk_plus, idx=idx_k, lr=0.01)\n",
        "    res_error.append(residual_error(sgd_xk_plus,x_star))\n",
        "\n",
        "  return (sgd_xk_plus,res_error)\n",
        "\n",
        "print(sgd_xk_plus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epFZbSFSCEiz"
      },
      "source": [
        "# Test and compare SGD and GD for different miniwebs\n",
        "* Monitor the residual error decay.\n",
        "* Test different step-sizes, and compare with the theory (in particular plot in log domain to illustrate the linear rate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF41wTfNCEiz"
      },
      "outputs": [],
      "source": [
        "## Insert your code here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwo3AGRYCEi0"
      },
      "source": [
        "# Compare running times\n",
        "Compare running times for increasingly large graphs with a fixed sparsity rate and for different implementations of PageRank (GD and SGD with sparse vs dense $A$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R2o19IsCEi0"
      },
      "outputs": [],
      "source": [
        "## Insert your code here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}